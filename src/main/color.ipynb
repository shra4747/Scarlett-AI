{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/Users/shravanprasanth/Documents/AIScouter/runs/detect/predict/crops/Robot/test5.jpg')\n",
    "\n",
    "height, width, _ = image.shape\n",
    "start_row = height // 2\n",
    "\n",
    "bottom_half_img = image[start_row:height, :]\n",
    "cv2.imwrite('bottom_half.jpg', bottom_half_img)\n",
    "cv2.imshow('Bottom Half', bottom_half_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rgb = np.mean(bottom_half_img, axis=(0, 1))\n",
    "\n",
    "print('Average RGB:', average_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install levenshtein\n",
    "!pip3 install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model (ensure your model is correctly trained for detecting robots)\n",
    "model = YOLO('/Users/shravanprasanth/Documents/AIScouter/src/models/y8v7.pt')  # Replace with your custom model if applicable\n",
    "\n",
    "# Open the input video\n",
    "input_video_path = '/Users/shravanprasanth/Documents/AIScouter/src/videos/dcmp58.mp4'\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video\n",
    "output_video_path = '/Users/shravanprasanth/Documents/AIScouter/src/videos/output/dcmp58-colors.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process each frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO detection on the frame\n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Get detected boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # Extract the bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "            # Crop the bottom half of the bounding box\n",
    "            start_row = (y1 + y2) // 2\n",
    "            bottom_half = frame[start_row:y2, x1:x2]\n",
    "\n",
    "            # Calculate the average RGB values\n",
    "            average_rgb = np.mean(bottom_half, axis=(0, 1))\n",
    "\n",
    "            # Check the RGB condition for labeling\n",
    "            label = 'Blue' if average_rgb[0] > average_rgb[-1] else 'Red'\n",
    "            color = (255, 0, 0) if label == 'Blue' else (0, 0, 255)\n",
    "\n",
    "            # Draw the bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the video objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Processing complete. Output saved to\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Load the YOLOv8 model (ensure your model is correctly trained for detecting robots)\n",
    "model = YOLO('/Users/shravanprasanth/Documents/AIScouter/src/models/y8v7.pt')\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Define the team numbers for red and blue teams\n",
    "red_team_numbers = [\"75\", \"2722\", \"1391\"]  # Replace with actual red team numbers\n",
    "blue_team_numbers = [\"56\", \"5401\", \"8513\"]  # Replace with actual blue team numbers\n",
    "\n",
    "# Open the input video\n",
    "input_video_path = '/Users/shravanprasanth/Documents/AIScouter/src/videos/dcmp58.mp4'\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the output video\n",
    "output_video_path = '/Users/shravanprasanth/Documents/AIScouter/src/videos/output/dcmp58-leven.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Process each frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO detection on the frame\n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Get detected boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # Extract the bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "            # Crop the bottom half of the bounding box\n",
    "            start_row = (y1 + y2) // 2\n",
    "            bottom_half = frame[start_row:y2, x1:x2]\n",
    "\n",
    "            # Calculate the average RGB values\n",
    "            average_rgb = np.mean(bottom_half, axis=(0, 1))\n",
    "\n",
    "            # Determine the team color based on RGB values\n",
    "            label = 'Blue' if average_rgb[0] > average_rgb[-1] else 'Red'\n",
    "            color = (255, 0, 0) if label == 'Blue' else (0, 0, 255)\n",
    "\n",
    "            # OCR: Recognize text in the bottom half of the box\n",
    "            ocr_result = reader.readtext(bottom_half)\n",
    "            detected_team_number = None\n",
    "\n",
    "            if ocr_result:\n",
    "                # Get the detected text\n",
    "                detected_text = ocr_result[0][-2].replace(\" \", \"\")  # Extract and clean text\n",
    "\n",
    "                # Find the closest matching team number using Levenshtein distance\n",
    "                if label == 'Red':\n",
    "                    closest_match = min(red_team_numbers, key=lambda num: levenshtein_distance(detected_text, num))\n",
    "                else:\n",
    "                    closest_match = min(blue_team_numbers, key=lambda num: levenshtein_distance(detected_text, num))\n",
    "\n",
    "                detected_team_number = closest_match\n",
    "\n",
    "            # Draw the bounding box and label on the frame\n",
    "            if detected_team_number:\n",
    "                label += f\" {detected_team_number}\"\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the video objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Processing complete. Output saved to\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from difflib import SequenceMatcher\n",
    "from rapidfuzz.distance import JaroWinkler\n",
    "\n",
    "# Load the YOLOv8 model (ensure your model is correctly trained for detecting robots)\n",
    "model = YOLO('/Users/shravanp/Coding/Robotics/AIScouter/src/models/y8v7.pt')\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Define the team numbers for red and blue teams\n",
    "red_team_numbers = [\"75\", \"2722\", \"1391\"]  # Replace with actual red team numbers\n",
    "blue_team_numbers = [\"56\", \"5401\", \"8513\"]  # Replace with actual blue team numbers\n",
    "\n",
    "# Provide the path to the input image\n",
    "input_image_path = '/Users/shravanp/Coding/Robotics/AIScouter/src/images/test.png'\n",
    "output_image_path = '/Users/shravanp/Coding/Robotics/AIScouter/src/images/output/test.png'\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(input_image_path)\n",
    "\n",
    "# Run YOLO detection on the image\n",
    "results = model(image)\n",
    "x = None\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Get detected boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        # Extract the bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "        # Crop the bottom half of the bounding box\n",
    "        start_row = (y1 + y2) // 2\n",
    "        bottom_half = image[start_row:y2, x1:x2]\n",
    "\n",
    "        gray = cv2.cvtColor(bottom_half, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply GaussianBlur to reduce noise and improve OCR accuracy\n",
    "\n",
    "        # # Apply thresholding to binarize the image\n",
    "        # # _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # # Apply morphological operations (optional, depends on image quality)\n",
    "        # # kernel = np.ones((2, 2), np.uint8)\n",
    "        # # morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(10, 10))\n",
    "        enhanced_gray = clahe.apply(gray)\n",
    "        upscale_model = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "        upscale_model.readModel('/Users/shravanp/Coding/Robotics/AIScouter/src/models/ESPCN_x4.pb')  # Path to the pre-trained model\n",
    "        upscale_model.setModel('espcn', 4)  # 'ESPCN' model with scale factor 4\n",
    "\n",
    "        # Upscale the image\n",
    "        upscaled_image = upscale_model.upsample(enhanced_gray)\n",
    "\n",
    "        # Sharpen the image using an unsharp mask\n",
    "        # gaussian_blur = cv2.GaussianBlur(enhanced_gray, (9, 9), 10.0)\n",
    "\n",
    "        # Calculate the average RGB values\n",
    "        average_rgb = np.mean(bottom_half, axis=(0, 1))\n",
    "\n",
    "        # Determine the team color based on RGB values\n",
    "        label = 'Blue' if average_rgb[0] > average_rgb[-1] else 'Red'\n",
    "        color = (255, 0, 0) if label == 'Blue' else (0, 0, 255)\n",
    "\n",
    "        # OCR: Recognize text in the bottom half of the box\n",
    "        # upscaled_img = cv2.resize(upscaled_image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "\n",
    "        assigned_teams = {}\n",
    "\n",
    "            if detected_text:\n",
    "                # Calculate Levenshtein distances and find the closest match\n",
    "                distances = {num: levenshtein_distance(detected_text, num) for num in team_numbers}\n",
    "                closest_match, min_distance = min(distances.items(), key=lambda item: item[1], default=(None, None))\n",
    "\n",
    "                if min_distance is not None:  # No threshold, just check if a match was found\n",
    "                    detected_team_number = closest_match\n",
    "                    team_numbers.remove(detected_team_number)\n",
    "                else:\n",
    "                    detected_team_number = \"Unknown\"\n",
    "            else:\n",
    "                detected_team_number = \"Unknown\"\n",
    "\n",
    "            # Update the assignment dictionary\n",
    "            assigned_teams[(x1, y1, x2, y2)] = detected_team_number\n",
    "\n",
    "            # Draw the bounding box and label on the image\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, f\"{detected_team_number}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "        # Handle any remaining team numbers\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2, _, _ = box\n",
    "            if assigned_teams.get((x1, y1, x2, y2)) == \"Unknown\":\n",
    "                if team_numbers:\n",
    "                    # Assign the next available team number\n",
    "                    detected_team_number = team_numbers.pop(0)\n",
    "                else:\n",
    "                    # Handle cases where no team numbers are available\n",
    "                    detected_team_number = \"Unknown\"\n",
    "\n",
    "                # Update the assignment dictionary and draw on the image\n",
    "                assigned_teams[(x1, y1, x2, y2)] = detected_team_number\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, f\"{detected_team_number}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Save the processed image\n",
    "cv2.imwrite(output_image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalization\n",
    "def normalize_image(img):\n",
    "    norm_img = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n",
    "    normalized_img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    return np.uint8(normalized_img)\n",
    "\n",
    "\n",
    "# 3. Image Scaling\n",
    "def set_image_dpi(file_path):\n",
    "    im = Image.open(file_path)\n",
    "    length_x, width_y = im.size\n",
    "    factor = min(1, float(1024.0 / length_x))\n",
    "    size = int(factor * length_x), int(factor * width_y)\n",
    "    im_resized = im.resize(size, Image.ANTIALIAS)\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')\n",
    "    temp_filename = temp_file.name\n",
    "    im_resized.save(temp_filename, dpi=(300, 300))\n",
    "    return temp_filename\n",
    "\n",
    "# 4. Noise Removal\n",
    "def remove_noise(image):\n",
    "    return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15)\n",
    "\n",
    "# 5. Thinning and Skeletonization\n",
    "def thin_image(image):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "# 6. Grayscale Conversion\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # 7. Thresholding or Binarization\n",
    "# def thresholding(image):\n",
    "#     return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "image_path = \"/Users/shravanp/Coding/Robotics/AIScouter/src/images/75.png\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "upscale_model = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "upscale_model.readModel('/Users/shravanp/Coding/Robotics/AIScouter/src/models/ESPCN_x4.pb')  # Path to the pre-trained model\n",
    "upscale_model.setModel('espcn', 4)  # 'ESPCN' model with scale factor 4\n",
    "\n",
    "# Upscale the image\n",
    "upscaled_image = upscale_model.upsample(image)\n",
    "print(reader.readtext(upscaled_image, allowlist=\"0123456789\", contrast_ths=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_team_numbers = [\"75\", \"2722\", \"1391\"] \n",
    "blue_team_numbers = [\"56\", \"5401\", \"8513\"]\n",
    "closest_match = max(red_team_numbers, key=lambda num: JaroWinkler.similarity(\"C\", num))\n",
    "closest_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ncnn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NCNN with Vulkan backend (usually Vulkan is enabled by default if supported)\n",
    "net = ncnn.Net()\n",
    "net.opt.use_vulkan_compute = False  # Enable Vulkan\n",
    "\n",
    "# Load the model\n",
    "net.load_param(\"/Users/shravanp/Coding/Robotics/AIScouter/src/models/models-DF2K/x4.param\")\n",
    "net.load_model(\"/Users/shravanp/Coding/Robotics/AIScouter/src/models/models-DF2K/x4.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input image\n",
    "img = cv2.imread(\"/Users/shravanp/Coding/Robotics/AIScouter/src/images/1391.png\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w, c = img.shape\n",
    "\n",
    "# Convert image to NCNN Mat\n",
    "input_img = ncnn.Mat.from_pixels(img, ncnn.Mat.PixelType.PIXEL_BGR, w, h)\n",
    "\n",
    "# Create NCNN extractor\n",
    "ex = net.create_extractor()\n",
    "ex.input(\"data\", input_img)\n",
    "\n",
    "# Run inference\n",
    "output = ncnn.Mat()\n",
    "ex.extract(\"output\", output)\n",
    "\n",
    "output_img = np.zeros((output.h, output.w, 3), dtype=np.uint8)\n",
    "\n",
    "# Convert NCNN Mat to numpy array by accessing each pixel channel\n",
    "for y in range(output.h):\n",
    "    for x in range(output.w):\n",
    "        output_img[y, x, 0] = output.channel(0)[y * output.w + x]\n",
    "        output_img[y, x, 1] = output.channel(1)[y * output.w + x]\n",
    "        output_img[y, x, 2] = output.channel(2)[y * output.w + x]\n",
    "\n",
    "# Save the output image\n",
    "output_img = cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
    "Image.fromarray(np.array(output_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_distance(\"1301\", [\"1391\", \"75\", \"2722\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
